{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Regression \n",
    "### CS 4262/5262 (Foundations of Machine Learning)<br><br>Vanderbilt University, Spring 2024\n",
    "---\n",
    "In this first assignment, you will have a chance to implement linear and polynomial regression models. In addition to programming tasks (marked with #TODO), there are some short-answer questions throughout the notebook. Please do not hesitate to ask for clarification. \n",
    "\n",
    "You are expected to clearly comment your code throughout the assignment.\n",
    "\n",
    "ChatGPT and other online resources may be used to assist you with the programming components. However, (i) the short-answer reflection questions should be done without using such resources; (ii) it is highly recommended that you nonetheless ensure that you are capable of writing all code on your own; and (iii) collaborators and all external resources must be listed at the end of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please enter your name:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Dataset\n",
    "This exercise will draw upon a portion of the California Housing dataset. Using this dataset, we will build models that predict the median price of a house given the average number of rooms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import random\n",
    "import sklearn.datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fetches data of interest:\n",
    "@return average number of rooms in house (numpy.ndarray), median price of house (numpy.ndarray)\n",
    "'''\n",
    "def fetch_housing_data():\n",
    "    housing_data = sklearn.datasets.fetch_california_housing()\n",
    "    # Note, we will use only a subset of the data (600 points) in this exercise\n",
    "    num_rooms = housing_data.data[400:1000,2]\n",
    "    median_price = housing_data.target[400:1000]\n",
    "    return num_rooms, median_price \n",
    "\n",
    "'''\n",
    "Renders a simple (x,y) plot. \n",
    "@param x (numpy.ndarray) - average number of rooms \n",
    "       y (numpy.ndarray) - median price of house \n",
    "'''\n",
    "def simple_plot(x,y):\n",
    "    plt.scatter(x,y)\n",
    "    plt.title('California Housing')\n",
    "    plt.xlabel('Average Number of Rooms')\n",
    "    plt.ylabel('Median Price (1000s)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the two functions above to display a plot of the data. Based on the plot, do you think linear regression will be able to effectively model the data? Why or why not?**\n",
    "\n",
    "Response:<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Run the functions here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_train_test()` function below will split the data into a training set and test set. Run the cell (no need to call the function yet). You may also wish to review the definition of overfitting; make sure you understand why it is important to have both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Partitions a dataset into a training set and a test set (80/20 split, respectively). \n",
    "Shuffles dataset before splitting. \n",
    "@param x - data inputs (numpy.ndarray)\n",
    "       y - data targets (numpy.ndarray)\n",
    "@return training set inputs, \n",
    "        training set targets, \n",
    "        test set inputs,\n",
    "        test set targets (all numpy.ndarray)\n",
    "'''\n",
    "def split_train_test(x, y):\n",
    "    xy = list(zip(x, y))\n",
    "    random.shuffle(xy)\n",
    "    x, y = zip(*xy)\n",
    "    \n",
    "    split = int(len(x)*0.8)\n",
    "    train_x = np.array(x[:split])\n",
    "    train_y = np.array(y[:split])\n",
    "    test_x = np.array(x[split:])\n",
    "    test_y = np.array(y[split:])\n",
    "    \n",
    "    train_y = np.reshape(train_y, (len(train_y),1))\n",
    "    test_y = np.reshape(test_y, (len(test_y),1))\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Simple Linear Regression\n",
    "For the `SimpleLinearRegressionModel` class below, fill in the incomplete methods using their respective descriptions. All methods you should fill out are marked with a #TODO. This model uses a Mean Squared Error cost function and gradient descent to optimize the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegressionModel():\n",
    "    \n",
    "    '''\n",
    "    Implementation of simple linear regression using gradient descent\n",
    "    @param x (numpy.ndarray) - training set of 'single-feature' inputs\n",
    "           y (numpy.ndarray) - training set of corresponding targets\n",
    "           theta (numpy.ndarray) - model parameters in the form of [intercept, coefficient]\n",
    "           alpha (float) - gradient descent step size\n",
    "    '''\n",
    "    def __init__(self, x, y, theta, alpha):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "     \n",
    "    '''\n",
    "    Equation for the regression line. \n",
    "    input x_i (float) - single input feature\n",
    "    @return corresponding model output (float)\n",
    "    '''\n",
    "    #TODO \n",
    "    def h(self, x_i):\n",
    "        return \n",
    "    \n",
    "    '''\n",
    "    Renders a plot of the training data and the regression line based on current model parameters.\n",
    "    ''' \n",
    "    def plot_current_model(self):\n",
    "        reg_line_y = [self.h(x_i) for x_i in self.x]\n",
    "        plt.scatter(self.x, self.y)\n",
    "        plt.plot(self.x, reg_line_y, color='green')\n",
    "        plt.title('Housing Dataset')\n",
    "        plt.xlabel('Average Number of Rooms')\n",
    "        plt.ylabel('Median Price')\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    Loss function measuring mean squared error of the regression line for a given training set and model parameters. \n",
    "    @return MSE based on the current parameters (float)\n",
    "    '''\n",
    "    #TODO\n",
    "    def J(self):\n",
    "        return \n",
    "    \n",
    "    '''\n",
    "    Update the model parameters (i.e. the two theta values) for one gradient descent step. Hint: this involves \n",
    "    computing partial derivatives. \n",
    "    '''\n",
    "    #TODO\n",
    "    def gradient_descent_step(self):\n",
    "        return\n",
    "        \n",
    "    '''\n",
    "    Run gradient descent to optimize the model parameters.\n",
    "    Keep track of the value of the cost function. You may change the default threshold for convergence.\n",
    "    @param threshold (float) - run gradient descent until the magnitude of the gradient is below this value. \n",
    "    @return a list storing the value of the cost function after every step of gradient descent (float list)\n",
    "    '''\n",
    "    #TODO\n",
    "    def run_gradient_descent(self, threshold=0.001):\n",
    "        return\n",
    "    \n",
    "    '''\n",
    "    Renders plot of MSE at each iteration of gradient descent\n",
    "    @param losses (float list) - MSE after every gradient descent step \n",
    "    '''\n",
    "    def plot_MSE_loss(self, losses):\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.title('Learning Curve')\n",
    "        plt.xlabel('Number of Steps')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying this model to the California housing dataset, let's test your code on a smaller dataset. In the debugging function below, first create a small set of 25 training examples ((x,y) pairs) that have an approximately linear relationship. You can do so by first generating pairs that have a perfect linear relationship, then adding a bit of random noise. Then, run your Simple Linear Regression model on this dataset and display the plot of the final regression line, superimposed on the 25 datapoints (call the `plot_current_model()` method). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function for debugging the simple linear regression model.\n",
    "'''\n",
    "#TODO\n",
    "def debug_SLR_model():\n",
    "    return\n",
    "\n",
    "debug_SLR_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, your model looks like it'll work. Next, partition the housing data into a training set and a test set (use the `split_train_test()` function from Part 1). Then, run the training set through new instances of `SimpleLinearRegression`. Try a value within the [$-40$,$-20$] range for $\\theta_0$ and a value within the [$0$,$20$] range for $\\theta_1$.\n",
    "\n",
    "Use the `plot_MSE_loss()` method to display the learning curve for three different step sizes (alpha): 0.001, 0.01, and 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the different step sizes here (next four cells).\n",
    "train_x, train_y, test_x, test_y = split_train_test(rooms, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = 0.001\n",
    "#TODO - run an instance of the model and plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha2 = 0.01\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha3 = 0.1\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your experiments, report a value of alpha and an initial value of $\\theta$ that allowed gradient descent to find a good solution (i.e., parameters $\\theta$ that fit the training data well) in a reasonable amount of time. Next, using the `h(x)` method together with your trained model, predict the target values of the test set ('median price') based on the input features of the test set ('average number of rooms'). Finally, for each datapoint in the test set, display a scatter plot of 'average number of rooms' (x-axis) against both the actual 'median house price' value and the model's prediction (y-axis).\n",
    "\n",
    "Use different colors for the actual vs. predicted values. Include a legend, title, and axis labels (see `simple_plot()` from Part 1 for example code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data here (next two cells). \n",
    "\n",
    "'''\n",
    "Make predictions for inputs using a trained SLR model. \n",
    "@param model (SimpleLinearRegression) - SLR model with (ideally) optimized parameters\n",
    "       test_x (numpy.ndarray) - inputs to run through the model \n",
    "@return corresponding predictions for the inputs (numpy.ndarray)\n",
    "'''\n",
    "#TODO\n",
    "def SLR_predict(model, test_x):\n",
    "    return\n",
    "\n",
    "'''\n",
    "Renders a plot showing the actual vs SLR-predicted outputs for a set of inputs. \n",
    "@param test_x (numpy.ndarray) - model inputs\n",
    "       test_y (numpy.ndarray) - corresponding actual outputs \n",
    "       pred_y (numpy.ndarray) - corresponding predicted outputs\n",
    "'''\n",
    "#TODO\n",
    "def plot_actual_vs_pred(test_x, test_y, pred_y):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "theta = None \n",
    "alpha = None \n",
    "\n",
    "slr = SimpleLinearRegressionModel(train_x, train_y, theta, alpha)\n",
    "losses = slr.run_gradient_descent()\n",
    "pred_y = SLR_predict(slr, test_x)\n",
    "plot_actual_vs_pred(test_x, test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Polynomial Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, we modeled the data with a linear function. Now, we will build a polynomial regression model and apply it to the same housing dataset. We will implement gradient descent, and vectorize operations whenever possible.\n",
    "\n",
    "In class, we learned that the gradient descent update rule can be written as:\n",
    "   $\\theta := \\theta - \\alpha \\cdot \\nabla J(\\theta)$,\n",
    "   and that for Batch Gradient Descent, the $j^{th}$ entry of $\\nabla_\\theta J(\\theta)$ can be expressed as:\n",
    " \n",
    "   $$\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `PolynomialRegressionModel` class below, note that there is a new field `self.degree` in the `init()` method, which specifies the highest polynomial degree in your regression equation. For example, if `self.degree=3`, then the regression equation is $y_i = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\theta_3 x_i^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, make sure you understand why polynomial regression be considered a case of multiple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**First,** complete the function `form_design_matrix()` below. This function should return the design matrix $X$ for polynomial regression, which is a matrix of dimensions $m \\times (d+1)$, where $d$ is the degree of the polynomial. Remember to include the intercept term (i.e., append the 'feature' $x_0 = 1$ to each training example, such that the first column of $X$ consists of all 1's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts an array of training set inputs into a design matrix, where rows represent \n",
    "training inputs and columns represent input features. \n",
    "@param training_inputs (numpy.ndarray) - training set of input features\n",
    "       degree (int) - highest polynomial degree to extend the design matrix into\n",
    "@return design matrix including the x_0 'feature' - (numpy.ndarray)\n",
    "'''\n",
    "#TODO\n",
    "def form_design_matrix(training_inputs, degree):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next,** before writing any further code, show that the above expression for the jth entry of the gradient ( = $\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} $) indeed corresponds to the jth entry of $\\nabla_\\theta J(\\theta) = X^T(X\\theta - y)$. You can write your derivation below using LaTeX, or upload a separate sheet of paper with a hand-written derivation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Show your derivation here, or upload derivation as a separate document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then,** complete the methods in the `PolynomialRegressionModel` class using vectorization. Assume that the model's training input will be in the form of a design matrix. A lot of your SLR code can be copied over to this new class— modify your methods accordingly to work for any $d$-degree polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegressionModel():\n",
    "    \n",
    "    '''\n",
    "    Implementation of a Polynomial regression model using MSE and Gradient Descent.\n",
    "    @param X (numpy.ndarray) - training set of input features, as a design matrix\n",
    "           y (numpy.ndarray) - training set of corresponding targets\n",
    "           theta (numpy.ndarray) - model parameters (variable coefficients, in order of increasing variable degree)\n",
    "           alpha (float) - learning step size\n",
    "           degree (int) - highest polynomial degree\n",
    "    '''\n",
    "    def __init__(self, X, y, theta, alpha, degree):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "        self.degree = degree\n",
    "             \n",
    "    '''\n",
    "    Hypothesis - return model prediction \n",
    "    @param X (numpy.ndarray) - design matrix of input features\n",
    "    @return corresponding model output (numpy.ndarray) \n",
    "    '''\n",
    "    #TODO\n",
    "    def h(self, X):\n",
    "        return\n",
    "    \n",
    "    '''\n",
    "    Renders a plot of the training data and the regression line based on current model parameters.\n",
    "    ''' \n",
    "    def plot_current_model(self):\n",
    "        reg_curve_y = self.h(self.X)\n",
    "        plt.scatter(self.X[:,1], self.y)\n",
    "        plt.scatter(self.X[:,1], reg_curve_y, color='green')\n",
    "        plt.title('California Housing (scaled data)')\n",
    "        plt.xlabel('Average Number of Rooms')\n",
    "        plt.ylabel('Median Price')\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    Cost function measuring mean squared error of the regression line for a given training set and model parameters.\n",
    "    Vectorize your code - no loops!\n",
    "    @return MSE based on the current parameters (float)\n",
    "    '''\n",
    "    #TODO\n",
    "    def J(self):\n",
    "        return\n",
    "    \n",
    "    '''\n",
    "    Update theta for one gradient descent step. Vectorize your code - no loops!\n",
    "    @return the gradient of the cost function (numpy.ndarray), for use in run_gradient_descent\n",
    "    '''\n",
    "    #TODO\n",
    "    def gradient_descent_step(self):\n",
    "        return\n",
    "       \n",
    "    '''\n",
    "    Run gradient descent to optimize the model parameters.\n",
    "    Keep track of the losses. You may change the default threshold for convergence. \n",
    "    Here, we will use a convergence criterion based on the norm of the gradient vector.\n",
    "    @param threshold (float) - run gradient descent until the absolute norm of the gradient is below this value.\n",
    "    @return a list storing the value of the cost function after every step of gradient descent (float list)\n",
    "    '''\n",
    "    def run_gradient_descent(self, threshold=0.001):\n",
    "        losses = []\n",
    "        loss = self.J()\n",
    "        losses.append(loss)\n",
    "    \n",
    "        norm_grad_vec = 1\n",
    "        while norm_grad_vec > threshold:\n",
    "            grad_vec = self.gradient_descent_step()\n",
    "            loss = self.J()\n",
    "            losses.append(loss)\n",
    "            norm_grad_vec = np.linalg.norm(grad_vec)\n",
    "        return losses\n",
    "    \n",
    "    '''\n",
    "    Renders the learning curve of the model during its optmization process. \n",
    "    @param losses (float list) - MSE after every gradient descent step \n",
    "    '''\n",
    "    def plot_MSE_loss(self, losses):\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.title('Learning Curve')\n",
    "        plt.xlabel('Number of Steps')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run an instance of the `PolynomialRegressionModel` class with a degree of 3. As feature scaling will help our model to converge more quickly, we can first scale our design matrix (normalize each column to have zero mean, unit variance - except for the column of ones). Here, let's set up the design matrix and perform this feature scaling (you can just execute this code, you do not need to write anything here - but make sure you understand what the code is doing). The resulting scaled design matrix will be your input to `PolynomialRegressionModel` in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "design_mat = form_design_matrix(train_x, degree)\n",
    "Xz = sc.stats.zscore(design_mat[:,1:]) # scale *except* for the column of ones\n",
    "m = Xz.shape[0]\n",
    "ones_vec = np.ones((m,1))\n",
    "design_mat_scaled = np.hstack((ones_vec,Xz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now,** run an instance of the `PolynomialRegressionModel` class using this design matrix. Experiment with the parameter 'alpha' and the initial value of the vector 'theta'. Plot the data and resulting regression curve using the method `plot_current_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Run an instance of the Polynomial Regression Model with a degree of 3 on the California housing data here. \n",
    "theta_init = np.array([20, -80, 160, -80.0]) # here is an example starting point that will converge quickly.\n",
    "theta_init = np.reshape(theta_init, (degree+1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you foresee will happen if you apply this model to the held-out test set?**\n",
    "\n",
    "Response:<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lastly,** as an alternative to running gradient descent for linear regression, one can solve directly for the value of $\\theta$ that minimizes the least-squares cost function $J(\\theta)$, using the equation:\n",
    "$$ \\theta = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "Please fill in the method `closed_form_solution()` below, using this equation. Run this using the feature-scaled design matrix constructed above, and report the value of $\\theta$ it returns, and compare it to the solution you obtained using gradient descent. \n",
    "\n",
    "Note:  your predictions will be the same regardless of whether you use the scaled or non-scaled version of the design matrix (optionally, you can check this on your own!). However, for the sake of comparing the values of $\\theta$, you can use the feature-scaled design matrix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Equation for directly solving for theta which minimizes the least-squares cost function J(theta).\n",
    "@param X (numpy.ndarray) - model input in the form of a design matrix \n",
    "       y (numpy.ndarray) - model output \n",
    "@return set of theta values for the regression equation that minimizes J(theta) (numpy.ndarray)\n",
    "'''\n",
    "#TODO \n",
    "def closed_form_solution(X, y):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - run closed_form_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent theta compared to closed-form solution theta:**<br>\n",
    "\n",
    "Response:<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about potential scenario(s) in which one might choose to use gradient descent over solving the closed-form solution. (no response required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Explore your own dataset \n",
    "\n",
    "\n",
    "For this part, you will investigate a dataset of your choosing! Please select a dataset that is suitable for regression analysis. You might choose to download a publicly available dataset, or one that you're using in your research, or one that you have assembled from real-world observations (do not just generate simulated data). Make sure that the dataset contains at least 100 datapoints. If your dataset is very large, you may use a subset of datapoints for this assignment so that the computational load is manageable.\n",
    "\n",
    "For the questions below, you may call any of the functions you wrote above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.1) Describe the dataset. What is the source of the data? What are some of the features in this dataset, and what target variable might you focus on predicting using regression? (If the dataset has several potential target variables $y$, you are free to select/define one.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response**:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.2) Load the data. Randomly select 75% of the datapoints to serve as the training set; the remaining 25% will comprise the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.3) Choose one particular input feature ($x$), and one target variable ($y$). Using the **training set only**, display a scatterplot of $x$ against $y$, and label the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - your code and plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.4) Based on eyeballing your plot, what degree of polynomial do you think might best describe the relationship between $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.5) Using your training set, fit a polynomial regression model to this data, using the model order that you estimated above. You may use either gradient descent or the closed-form solution to fit the model. Superimpose the fitted regression curve onto the scatterplot of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - your code and plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.6) Apply the regression model that you fit in (4.4) to make predictions on each point in the test set. Superimpose the fitted function on a scatterplot of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - your code and plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.7) How well did your model appear to fit the data? Calculate and report the mean-squared error on the test set. In addition, experiment with increasing and decreasing the model order, and describe what happens to the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - your code and plot(s) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Submission and resource declaration\n",
    "\n",
    "Once you're ready to submit, create a 'clean' version of your final solutions, removing any extra debugging code you may have written. Next, in the menu bar, click `Kernel > Restart & Clear Output`. Then run your code from top to bottom, so that all the plots are displayed. Back in the menu bar, click `File > Download as > Notebook (.ipynb)` to download your notebook. Don't forget to answer the short answer questions. \n",
    "\n",
    "Please upload this to Brightspace by midnight of the deadline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators and resources\n",
    "Please list all collaborators and external resources here: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
