{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Assignment 3: Classification: GDA and SVM\n",
    "</center>\n",
    "</h1>\n",
    "<center>\n",
    "CS 4262/5262 - Foundations of Machine Learning<br>\n",
    "Vanderbilt University, Spring 2024<br>\n",
    "Due: Check Brightspace\n",
    "</center>\n",
    "<hr>\n",
    "<br>This assignment will focus on Gaussian Discriminant Analysis and Support Vector Machines. In addition to programming tasks, there are short-answer questions throughout the notebook. \n",
    "\n",
    "Contact: Gary Chung kuan-i.chung@vanderbilt.edu for any clarifying questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**:\n",
    "\n",
    "**Course**: e.g. 4262\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Gaussian Discriminant Analysis (theoretical part)\n",
    "During the class we derive the closed-form solution for the parameters $\\phi$ and $\\mu_1$. In this part, you will need to derive the closed-form solution for $\\mu_0$ and $\\Sigma$. You can start your derivation from stating the log-likelihood function and then take the derivative with respect to $\\mu_0$ and $\\Sigma$. Also, you will need to show why the decision boundary is linear when the covariance matrix is the same for both classes. And finally, GDA can be seen as a special case of logistic regression.\n",
    "\n",
    "For 4262 students, you only need to do question 1, 3. For 5262 students, you need to finish all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Derive the closed-form solution for $\\mu_0$.\n",
    "\n",
    "**Ans.:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (optional for 4262):** Derive the closed-form solution for $\\Sigma$.\n",
    "\n",
    "**Ans.:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Show that the decision boundary for GDA is linear when the covariance matrices of the two classes are the same. Please derive in algebraic form. Something like\n",
    "$$a^T x + b = 0$$\n",
    "but showing $a^T$ and $b$ in terms of $\\Sigma$, $\\mu_0$, $\\mu_1$, and $\\phi$.\n",
    "\n",
    "**Ans.:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brainstorming: (no marks or deduction)** Why is GDA can be considered as a logistic regression model? That is, why the following equation holds?\n",
    "$$p(y=1|x) = \\frac{1}{1 + e^{-(a^Tx+b)}}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "a^T = (\\mu_1-\\mu_0)^T\\Sigma^{-1}\\\\\\\\\n",
    "b= \\frac{1}{2}\\mu_0^T\\Sigma^{-1}\\mu_0- \\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1 +\\log\\frac{\\phi}{1-\\phi}\n",
    "$$\n",
    "\n",
    "In the later part, you can use the above equation to help you implement the GDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Gaussian Discriminant Analysis (implementation part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "- Write a function (or a set of functions) that takes in a set of training data and returns the maximum likelihood estimates of the parameters $\\mu_0$, $\\mu_1$, $\\Sigma$, and $\\phi$. Assume that the class covariance matrices are equal, which results in a linear decision boundary. You can use the formulas provided in the lecture notes for the maximum likelihood estimates of each parameter. \n",
    "\n",
    "- Load the Wine dataset (the same files as Assignment2 are included in this distribution). Choose columns of citric acid and total sulfur dioxide as input $x$. \n",
    "- Splitting into training/test sets in 80/20 ratio.\n",
    "- Pick 50 samples for each class from the training set (100 in total).\n",
    "- Plot as follows:\n",
    "    - The training data points from each class in a scatter plot.\n",
    "    - Show the $\\mu_0$ and $\\mu_1$ on the plot.\n",
    "    - Giving different colors to the two classes.\n",
    "    - The decision boundary on the same plot.\n",
    "    - Set plotting limits to [-.08, 1.8] for citric acid and [-5, 360] for total sulfur dioxide.\n",
    "- Calculate and report the model performance on test set. Please make sure to report F1 Score as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - write functions to calculate the GDA parameters, and estimate these parameters on the wine dataset.\n",
    "\n",
    "class GDA:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mu_0 = self.get_mu_0()\n",
    "        self.mu_1 = self.get_mu_1()\n",
    "        self.sigma = self.get_sigma()\n",
    "        self.phi = self.get_phi()\n",
    "    \n",
    "    def get_mu_0(self):\n",
    "        pass\n",
    "\n",
    "    def get_mu_1(self):\n",
    "        pass\n",
    "\n",
    "    def get_sigma(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_phi(self):\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def a_t(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def b(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self, x_new, threshold=0.5):\n",
    "        z = x_new @ self.a_t + self.b\n",
    "        return self.sigmoid(z) > threshold\n",
    "    \n",
    "    def decision_boundary(self, x):\n",
    "        return -(self.b + self.a_t[0] * x) / self.a_t[1]\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.scatter(self.x[self.y == 0][:, 0],\n",
    "                    self.x[self.y == 0][:, 1],\n",
    "                    label='white wine', alpha=0.2, marker='.')\n",
    "        plt.scatter(self.x[self.y == 1][:, 0],\n",
    "                    self.x[self.y == 1][:, 1],\n",
    "                    label='red wine', alpha=0.2, marker='.')\n",
    "        x1 = np.linspace(self.x[:, 0].min(), self.x[:, 0].max(), 100)\n",
    "        x2 = self.decision_boundary(x1)\n",
    "        plt.plot(x1, x2, label='Decision Boundary', c='g')\n",
    "        plt.scatter(self.mu_0[0], self.mu_0[1], c='blue', marker='x', label='$\\mu_0$', s=80)\n",
    "        plt.scatter(self.mu_1[0], self.mu_1[1], c='brown', marker='x', label='$\\mu_1$', s=80)\n",
    "        plt.xlabel('citric acid')\n",
    "        plt.ylabel('total sulfur dioxide')\n",
    "        plt.title('GDA')\n",
    "        plt.xlim(-.08, .8)\n",
    "        plt.ylim(-5, 360)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def classification_report(self, y_true, y_pred):\n",
    "        unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "        report_dict = {}\n",
    "    \n",
    "        for label in unique_labels:\n",
    "            tp = np.sum((y_true == label) & (y_pred == label))\n",
    "            fp = np.sum((y_true != label) & (y_pred == label))\n",
    "            fn = np.sum((y_true == label) & (y_pred != label))\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            support = np.sum(y_true == label)\n",
    "            \n",
    "            report_dict[label] = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1-score\": f1_score,\n",
    "                \"support\": support\n",
    "            }\n",
    "        \n",
    "        # Optionally, compute and add overall averages to the report\n",
    "        macro_precision = np.mean([metrics[\"precision\"] for metrics in report_dict.values()])\n",
    "        macro_recall = np.mean([metrics[\"recall\"] for metrics in report_dict.values()])\n",
    "        macro_f1 = np.mean([metrics[\"f1-score\"] for metrics in report_dict.values()])\n",
    "    \n",
    "        return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset and split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment in Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "\n",
    "Increase to 1200 sample for class 1 only. Now we have 50 samples for class 0 and 1200 samples for class 1. And repeat the same process as in Task 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment in Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Since the plotting limits are set to the same range for both tasks, what do you observe about the decision boundary?\n",
    "\n",
    "**Ans.:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** What do you observe about the model performance on the test set when the number of samples for class 1 is increased? Why do you think this happens?\n",
    "\n",
    "**Ans.:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:**\n",
    "\n",
    "Coding Task: What do you think the accuracy will be when the test data is imblanced as well. Develop model as same previous case. (Training phase will be same as **task 1**). For testing just consider 200 positive labed records from test set (y=1) and 20 negative labed records(y=0). Report testing results. Write your analysis based on below code results. Please report F1 score. Discuss about your observations on F1 score and compare it with accuracy in case of imbalanced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Support Vector Machine\n",
    "\n",
    "Now, you will apply a Support Vector Machine (with radial basis function kernel) to the [Wisconsin Breast Cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). Use the columns 'perimeter (mean)' and 'symmetry (mean)' for the input features in your calculations. Here, rather than writing your own SVM class, you will be calling functions provided in scikit-learn: [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** Why is feature scaling important when using a support vector machine with the RBF kernel?\n",
    "\n",
    "**Ans.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "<br><br>Using the WBC dataset, shuffle the dataset, split it using a 80/20  train/test partition, and perform feature scaling. You may refer to your code from Assignment 2 for those steps. Refer to instructions regarding feature scaling in the Logistic Regression section of Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - process and partition the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** In the context of scikit-learn's SVM implementation (linked above), explain what hyperparameters C and gamma are, and describe the effects of increasing and decreasing their values.\n",
    "\n",
    "**Ans.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "\n",
    "You will implement k-fold cross-validation to select the SVM hyperparameters. Note: you must write k-fold CV yourself, do not use sklearn.model_selection.KFold. However, you may use scikit-learn's SVC (you don't need to implement support vector machine yourself).\n",
    "\n",
    "- Choose three values of C and three values of gamma that you wish to consider. Additionally, pick a value of _k_ (# of cross-validation folds) \n",
    "- For each pair of hyperparameter values (C, gamma), perform k-fold cross validation *within the training set* you designated above.  \n",
    "- Report the pair of hyperparameter values that yields the highest accuracy (averaged across the k iterations) on this k-fold CV.\n",
    "- Using that pair of hyperparameters, train a \"final\" SVM using the *entire* training set\n",
    "- Run and report the accuracy of this model on the held-out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - k-fold cross validation on SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** Which pair of hyperparameters were selected by k-fold CV? What was the accuracy of the corresponding model on the held-out test set?\n",
    "\n",
    "**Ans.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10:** Discuss the effects of changing C and gamma. To illustrate your response, generate plots of the decision boundary resulting from different values of C and gamma, in the following way:\n",
    "- Using the 'best' value of C (selected based on k-fold CV above), sweep over 2-3 different values of gamma, generating one plot of the decision boundary (superimposed on the training points) each time. Use the entire training set.\n",
    "- Repeat the above, this time using the 'best' value of gamma (selected based on k-fold CV above) and sweeping over 2-3 different values of C.\n",
    "\n",
    "**Ans.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code will plot the decision boundary on a dataset for a given classifier. \n",
    "# source: https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch02/ch02.ipynb\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # set up marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - analyze the effects of C and gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Submission \n",
    "\n",
    "Please upload a clean version of your work to Brightspace by the deadline. <em>If you use a separate PDF with your short answer questions, it should be added alongside the ipynb file as a PDF, and zipped up together as your solution.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, please acknowledge your collaborators as well as any resources/references (beyond guides to Python syntax) that you have used in this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
